(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[934],{1118:function(e,r,n){(window.__NEXT_P=window.__NEXT_P||[]).push(["/page3",function(){return n(3789)}])},3789:function(e,r,n){"use strict";n.r(r),n.d(r,{default:function(){return l}});var s=n(5893),t=n(7294),a=n(9008),i=n.n(a);function c(){let[e,r]=(0,t.useState)(!1);return(0,s.jsxs)("div",{className:"references-dropdown",children:[(0,s.jsx)("button",{onClick:()=>r(!e),className:"references-button",children:e?"▲ References & Copyright":"▼ References & Copyright"}),e&&(0,s.jsxs)("div",{className:"references-content",children:[(0,s.jsxs)("p",{children:[(0,s.jsx)("strong",{children:"This project is inspired by ChatGPT (OpenAI)"}),(0,s.jsx)("br",{}),"Reference:",(0,s.jsx)("a",{href:"https://chat.openai.com/",target:"_blank",rel:"noreferrer",children:"https://chat.openai.com/"})," "," / "," ",(0,s.jsx)("a",{href:"https://chatgpt.com/c/677f3cea-9d94-8000-be2e-a3571ea7a84b",target:"_blank",rel:"noreferrer",children:"https://chatgpt.com/c/677f3cea-9d94-8000-be2e-a3571ea7a84b"})]}),(0,s.jsx)("p",{children:"We express gratitude to OpenAI for design & LLM features (GPT-4.0)."}),(0,s.jsx)("p",{children:(0,s.jsx)("strong",{children:"Also referencing:"})}),(0,s.jsxs)("ul",{children:[(0,s.jsxs)("li",{children:["Vaswani et al. (2017).",(0,s.jsx)("em",{children:'"Attention Is All You Need."'}),(0,s.jsx)("br",{}),(0,s.jsx)("a",{href:"https://arxiv.org/abs/1706.03762",target:"_blank",rel:"noreferrer",children:"[arXiv:1706.03762]"})]}),(0,s.jsxs)("li",{children:["Brown et al. (2020).",(0,s.jsx)("em",{children:'"Language Models are Few-Shot Learners."'}),(0,s.jsx)("br",{}),(0,s.jsx)("a",{href:"https://arxiv.org/abs/2005.14165",target:"_blank",rel:"noreferrer",children:"[arXiv:2005.14165]"})]}),(0,s.jsx)("li",{children:"(その他参照した論文やリンクを適宜追加)"})]}),(0,s.jsx)("hr",{style:{margin:"1rem 0"}}),(0,s.jsx)("p",{style:{fontSize:"0.9rem"},children:(0,s.jsx)("em",{children:"Note: This source code does NOT contain any direct copy of ChatGPT's original source code, but is an independent implementation that recreates a similar interface & functionality."})})]})]})}function l(){return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(i(),{children:[(0,s.jsx)("meta",{charSet:"UTF-8"}),(0,s.jsx)("title",{children:"Page3 | 2017 Transformer + BlackRing"}),(0,s.jsx)("link",{rel:"stylesheet",href:"/css/kaleido3.css"})]}),(0,s.jsxs)("main",{className:"kaleidoMain3",children:[(0,s.jsx)("div",{className:"blackRingArea",children:Array.from({length:5}).map((e,r)=>(0,s.jsx)("div",{className:"ringArc arc".concat(r)},r))}),(0,s.jsxs)("section",{className:"frontContent3",children:[(0,s.jsx)("h2",{children:"2017 Attention Transformer Visualization"}),(0,s.jsxs)("p",{children:["Black ring arcs spinning around the center, depicting Q-K-V synergy.",(0,s.jsx)("a",{href:"https://arxiv.org/abs/1706.03762",target:"_blank",rel:"noreferrer",children:"[Attention Is All You Need]"})]}),(0,s.jsx)(c,{})]})]})]})}n(2195)},2195:function(){}},function(e){e.O(0,[774,888,179],function(){return e(e.s=1118)}),_N_E=e.O()}]);